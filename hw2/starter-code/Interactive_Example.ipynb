{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import Indexer, IndexType\n",
    "from document_preprocessor import RegexTokenizer\n",
    "from ranker import *\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from network_features import NetworkFeatures\n",
    "from l2r import L2RFeatureExtractor, L2RRanker\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you want to reload a class/method without restarting the whole notebook, use/modify this\n",
    "#       code to get the latest changes from your file.  You should change it to whatever file/class\n",
    "#       you want reloaded. The example is for the l2r file but you can use whatever you want.\n",
    "from importlib import reload\n",
    "import l2r \n",
    "reload(l2r)\n",
    "from l2r import L2RRanker, L2RFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the stopwords\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "print('Loaded %d stopwords' % len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of categories for each page (either compute it or load the pre-computed list)\n",
    "docid_to_categories = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or pre-compute the list of categories at least the minimum number of times (specified in the homework)\n",
    "category_counts = Counter()\n",
    "recognized_categories = set()\n",
    "print(\"saw %d categories\" % len(recognized_categories))\n",
    "\n",
    "# Map each document to the smallest set of categories that occur frequently\n",
    "doc_category_info = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_features = {}\n",
    "# Get or load the network statistics for the Wikipedia link network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or build Inverted Indices for the documents' main text and titles\n",
    "#\n",
    "# Estiamted times: \n",
    "#    Document text token counting: 3m40s\n",
    "#    Document text indexing: 4m45s\n",
    "#    Title text indexing: 22s\n",
    "preprocessor = RegexTokenizer('\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature extractor and an initial ranker for determining what to re-rank\n",
    "# Use these with a L2RRanker and then train that L2RRanker model\n",
    "#\n",
    "# Estimated time (using 4 cores via n_jobs): 2m40s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Try retrieving documents for a a new query\n",
    "#\n",
    "query_phrase = \"AI chatbots and vehicles\"\n",
    "results = ranker.query(query_phrase)\n",
    "print('Ranked documents:')\n",
    "for r in results[:15]:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
